{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2eab8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\Multi-modal RAG\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "import fitz \n",
    "from transformers import CLIPProcessor, CLIPModel \n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "from langchain.chat_models import init_chat_model \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import base64 \n",
    "import io\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa2c63",
   "metadata": {},
   "source": [
    "### **CLIP Model loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70387188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch    \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae9dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the CLIP model for Unified embeddings \n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f22416",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding function \n",
    "def image_embed(image_path):\n",
    "    \"Embed images using CLIP\"\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    else:\n",
    "        image = image_path \n",
    "\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad(): \n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        # Normalizing embeddings \n",
    "        features = features / features.norm(dim = -1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "    \n",
    "def text_embed(text):\n",
    "    \"Embed text using CLIP\"\n",
    "    inputs = clip_processor(text = text, return_tensors=\"pt\", padding = True, truncation = True, max_length = 77)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalizing Embeddings\n",
    "        features = features / features.norm(dim = -1, keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7281639",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process pdf \n",
    "pdf_path = r\"C:\\Users\\hites\\OneDrive\\Desktop\\Multi-modal RAG\\data\\BERT_Slides.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# Storage for docs and embeddings \n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}\n",
    "\n",
    "## Text splitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36815b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('C:\\Users\\hites\\OneDrive\\Desktop\\Multi-modal RAG\\data\\BERT_Slides.pdf')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d732845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page in enumerate(doc):\n",
    "    ## process text \n",
    "    text = page.get_text()\n",
    "    if text.strip():\n",
    "        # Create a temporary for splitting \n",
    "        temp_doc = Document(page_content=text, metadata={'page':i, 'type': 'text'})\n",
    "        text_chunks = text_splitter.split_documents([temp_doc])\n",
    "        \n",
    "        # Embed each chunks using CLIP \n",
    "        for chunk in text_chunks:\n",
    "            embedding = text_embed(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "        \n",
    "    ## Process Images \n",
    "    # 1. Convert PDF images to PIL Format \n",
    "    # 2. Store as base64 for GPT-4o model (which takes base64 images)\n",
    "    # Create CLIP Embeddings for retrieval \n",
    "    for img_index, img in enumerate(page.get_images(full = True)):\n",
    "        try:\n",
    "            xref = img[0] \n",
    "            base_image = doc.extract_image(xref=xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            # Convert to PIL Image \n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "            # Create Unique identifier \n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "\n",
    "            # Store image as base64 for later use with gpt-4o \n",
    "            buffered = io.BytesIO() \n",
    "            pil_image.save(buffered, format = \"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64 \n",
    "\n",
    "            # Embed image using CLIP \n",
    "            embedding = image_embed(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "            # Create document for image \n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\", \n",
    "                metadata = {\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue \n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9585c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "embedding_array = np.array(all_embeddings) \n",
    "\n",
    "# Creating custom FAISS index since we have precomputed embeddings \n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings = [(doc.page_content, emb) for doc, emb in zip(all_docs, all_embeddings)], \n",
    "    embedding=None, # using precomputed embeddings \n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e033e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1f425184160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332d0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(model=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e6c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_multimodal(query, k = 5):\n",
    "    \"\"\"Retrival using CLIP embeddings for both text and images.\"\"\"\n",
    "    # Embed query using CLIP \n",
    "    query_embedding = text_embed(query) \n",
    "\n",
    "    # Search in Vector store \n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding = query_embedding, \n",
    "        k = k \n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663d5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrived_docs):\n",
    "    \"\"\"Create a message using both text and images for GPT\"\"\" \n",
    "    content = [] \n",
    "\n",
    "    # Add the query \n",
    "    content.append({\n",
    "        \"type\": \"text\", \n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"  \n",
    "    })\n",
    "\n",
    "    # Seperate text and image document \n",
    "    text_docs = [doc for doc in retrived_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrived_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Add text context \n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    \n",
    "    # Add Images \n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Add Instruction \n",
    "    content.append({\n",
    "        'type': 'text', \n",
    "        'text': \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "\n",
    "    return HumanMessage(content = content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d04bec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrive_multimodal(query=query, k=1) \n",
    "\n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs) \n",
    "\n",
    "    # Get response from GPT-4\n",
    "    response = llm.invoke([message]) \n",
    "\n",
    "    # Print retrieved context info\n",
    "    # print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    # for doc in context_docs:\n",
    "    #     doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "    #     page = doc.metadata.get(\"page\", \"?\")\n",
    "    #     if doc_type == \"text\":\n",
    "    #         preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "    #         print(f\"  - Text from page {page}: {preview}\")\n",
    "    #     else:\n",
    "    #         print(f\"  - Image from page {page}\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5de13156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How to inference a language model\n",
      "--------------------------------------------------\n",
      "Answer: Certainly! Based on the provided text and context (\"How to inference a language model?\", mention of Transformer Encoder, input and output sequences, and appending the last token), here is how you inference a language model:\n",
      "\n",
      "**How to inference a language model:**\n",
      "\n",
      "1. **Start with an Input Sequence:**  \n",
      "   Provide an initial input sequence to the model (for example: `[SOS] Before my`), where `[SOS]` is the start-of-sequence token.\n",
      "\n",
      "2. **Pass Through the Language Model:**  \n",
      "   Feed this input into the language model (such as the Transformer Encoder).\n",
      "\n",
      "3. **Generate the Next Token:**  \n",
      "   The model predicts the next token in the sequence (e.g., it outputs `bed` so the sequence is now `[SOS] Before my bed`).\n",
      "\n",
      "4. **Append the Last Token:**  \n",
      "   Take the newly predicted token (`bed`) and append it to your input sequence.\n",
      "\n",
      "5. **Repeat as Needed:**  \n",
      "   Continue passing the extended input back into the model to generate further tokens, each time appending the last prediction, until you produce the complete output sequence or reach an end-of-sequence token.\n",
      "\n",
      "**Summary:**  \n",
      "You inference a language model by feeding it an initial input, generating the next token, appending it to the input, and repeating this process to generate sequences step-by-step.\n",
      "\n",
      "**References:**  \n",
      "- https://github.com/hkproj/bert-from-scratch  \n",
      "- The process as presented in the excerpt (\"Append the last token to the input\")\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "        \"How to inference a language model\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = multimodal_rag_pipeline(query)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f162f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
